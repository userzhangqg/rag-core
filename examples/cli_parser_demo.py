#!/usr/bin/env python3
"""
å‘½ä»¤è¡Œæ–‡ä»¶è§£ææ¼”ç¤ºç¨‹åº

è¿™ä¸ªdemoæ¼”ç¤ºå¦‚ä½•ä½¿ç”¨DocumentProcessingPipelineä»å‘½ä»¤è¡Œè¾“å…¥æ–‡ä»¶è¿›è¡Œè§£æï¼Œ
å¹¶æ‰“å°è¯¦ç»†çš„è§£æç»“æœï¼ŒåŒ…æ‹¬æ–‡æ¡£ç»“æ„ã€åˆ†å—ä¿¡æ¯ç­‰ã€‚
"""

import sys
import argparse
from pathlib import Path
import json
from typing import Dict, Any, List

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
import os
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from core.pipeline.preprocessing_pipeline import DocumentProcessingPipeline
from conf.config import RAGConfig
from utils.logger import setup_logger


def print_document_info(chunks: List[Dict[str, Any]], file_path: str, detailed: bool = False) -> None:
    """æ‰“å°æ–‡æ¡£è§£æä¿¡æ¯
    
    Args:
        chunks: æ–‡æ¡£åˆ†å—åˆ—è¡¨
        file_path: æ–‡ä»¶è·¯å¾„
        detailed: æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
    """
    print(f"\n{'='*80}")
    print(f"æ–‡ä»¶è§£æç»“æœ: {file_path}")
    print(f"{'='*80}")
    
    if not chunks:
        print("âš ï¸  æœªè§£æåˆ°ä»»ä½•å†…å®¹")
        return
    
    print(f"ğŸ“Š æ€»è®¡åˆ†å—æ•°é‡: {len(chunks)}")
    print()
    
    # æ‰“å°æ–‡ä»¶çº§åˆ«å…ƒæ•°æ®
    first_chunk = chunks[0]
    file_metadata = {
        k: v for k, v in first_chunk['metadata'].items() 
        if k in ['source_file', 'file_name', 'file_size', 'document_type']
    }
    
    print("ğŸ“ æ–‡ä»¶ä¿¡æ¯:")
    for key, value in file_metadata.items():
        print(f"   {key}: {value}")
    print()
    
    # æ‰“å°å®Œæ•´è§£ææ–‡æœ¬
    print("ğŸ“„ å®Œæ•´è§£ææ–‡æœ¬:")
    full_text = "\n".join([chunk['text'] for chunk in chunks])
    print(full_text)
    print()
    
    # å¦‚æœå¯ç”¨è¯¦ç»†æ¨¡å¼ï¼Œæ‰“å°æ¯ä¸ªåˆ†å—çš„è¯¦ç»†ä¿¡æ¯
    if detailed:
        # æ‰“å°æ¯ä¸ªåˆ†å—çš„è¯¦ç»†ä¿¡æ¯
        for idx, chunk in enumerate(chunks, 1):
            print(f"ğŸ” åˆ†å— #{idx}:")
            print(f"   æ–‡æœ¬é•¿åº¦: {len(chunk['text'])} å­—ç¬¦")
            
            # æ‰“å°åˆ†å—å…ƒæ•°æ®
            relevant_metadata = {
                k: v for k, v in chunk['metadata'].items() 
                if k not in ['source_file', 'file_name', 'file_size', 'document_type']
            }
            
            if relevant_metadata:
                print("   å…ƒæ•°æ®:")
                for key, value in relevant_metadata.items():
                    if key == 'text':
                        continue  # è·³è¿‡æ–‡æœ¬å†…å®¹
                    print(f"     {key}: {value}")
            
            # æ‰“å°æ–‡æœ¬å†…å®¹é¢„è§ˆ
            text_preview = chunk['text'][:200].replace('\n', ' ')
            if len(chunk['text']) > 200:
                text_preview += "..."
            print(f"   å†…å®¹é¢„è§ˆ: {text_preview}")
            print(f"{'-'*60}")


def print_pipeline_config(pipeline: DocumentProcessingPipeline) -> None:
    """æ‰“å°ç®¡é“é…ç½®ä¿¡æ¯"""
    print(f"\n{'='*80}")
    print("ç®¡é“é…ç½®ä¿¡æ¯")
    print(f"{'='*80}")
    
    info = pipeline.get_pipeline_info()
    
    print("ğŸ”§ è§£æå™¨é…ç½®:")
    for key, value in info['parser_config'].items():
        print(f"   {key}: {value}")
    
    print("\nâœ‚ï¸  åˆ†å—å™¨é…ç½®:")
    for key, value in info['chunker_config'].items():
        print(f"   {key}: {value}")
    
    print(f"\nğŸ·ï¸  å…ƒæ•°æ®å¯ç”¨: {info['enable_metadata']}")


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description="å‘½ä»¤è¡Œæ–‡ä»¶è§£ææ¼”ç¤ºç¨‹åº",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  python cli_parser_demo.py /path/to/document.pdf
  python cli_parser_demo.py /path/to/file.md --verbose
  python cli_parser_demo.py ./documents/ --recursive
  python cli_parser_demo.py test.docx --output result.json
        """
    )
    
    parser.add_argument(
        'input_path',
        help='è¦è§£æçš„æ–‡ä»¶è·¯å¾„æˆ–ç›®å½•è·¯å¾„'
    )
    
    parser.add_argument(
        '--recursive', '-r',
        action='store_true',
        help='é€’å½’å¤„ç†ç›®å½•ä¸­çš„æ–‡ä»¶'
    )
    
    parser.add_argument(
        '--pattern', '-p',
        default='*',
        help='æ–‡ä»¶åŒ¹é…æ¨¡å¼ï¼ˆä»…å¯¹ç›®å½•æœ‰æ•ˆï¼Œé»˜è®¤: *ï¼‰'
    )
    
    parser.add_argument(
        '--verbose', '-v',
        action='store_true',
        help='æ˜¾ç¤ºè¯¦ç»†æ—¥å¿—ä¿¡æ¯'
    )
    
    parser.add_argument(
        '--detailed', '-d',
        action='store_true',
        help='æ˜¾ç¤ºæ¯ä¸ªåˆ†å—çš„è¯¦ç»†ä¿¡æ¯'
    )
    
    parser.add_argument(
        '--output', '-o',
        help='å°†ç»“æœä¿å­˜åˆ°JSONæ–‡ä»¶'
    )
    
    parser.add_argument(
        '--config',
        help='ä½¿ç”¨è‡ªå®šä¹‰é…ç½®æ–‡ä»¶'
    )
    
    args = parser.parse_args()
    
    # è®¾ç½®æ—¥å¿—çº§åˆ«
    # if args.verbose:
    #     setup_logger(level='DEBUG')
    # else:
    #     setup_logger(level='INFO')
    
    try:
        # åˆå§‹åŒ–é…ç½®
        if args.config:
            config = RAGConfig.from_config_file(args.config)
        else:
            config = RAGConfig.from_config_file()

        print(config)
        setup_logger(config)
        
        # åˆ›å»ºå¤„ç†ç®¡é“
        pipeline = DocumentProcessingPipeline(config)
        
        # æ‰“å°é…ç½®ä¿¡æ¯
        print_pipeline_config(pipeline)
        
        input_path = Path(args.input_path)
        
        if not input_path.exists():
            print(f"âŒ é”™è¯¯: è·¯å¾„ä¸å­˜åœ¨: {input_path}")
            sys.exit(1)
        
        all_results = {}
        
        if input_path.is_file():
            # å¤„ç†å•ä¸ªæ–‡ä»¶
            print(f"\nğŸš€ å¼€å§‹è§£ææ–‡ä»¶: {input_path}")
            chunks = pipeline.process_file(str(input_path))
            print_document_info(chunks, str(input_path), args.detailed)
            all_results[str(input_path)] = chunks
            
        elif input_path.is_dir():
            # å¤„ç†ç›®å½•
            print(f"\nğŸš€ å¼€å§‹å¤„ç†ç›®å½•: {input_path}")
            results = pipeline.process_directory(
                str(input_path),
                file_pattern=args.pattern,
                recursive=args.recursive
            )
            
            for file_path, chunks in results.items():
                print_document_info(chunks, file_path, args.detailed)
                all_results[file_path] = chunks
        
        # ä¿å­˜ç»“æœåˆ°æ–‡ä»¶
        if args.output:
            output_data = {}
            for file_path, chunks in all_results.items():
                output_data[file_path] = [
                    {
                        'text': chunk['text'],
                        'metadata': chunk['metadata']
                    }
                    for chunk in chunks
                ]
            
            with open(args.output, 'w', encoding='utf-8') as f:
                json.dump(output_data, f, ensure_ascii=False, indent=2)
            
            print(f"\nâœ… ç»“æœå·²ä¿å­˜åˆ°: {args.output}")
    
    except KeyboardInterrupt:
        print("\nâš ï¸  ç”¨æˆ·ä¸­æ–­æ“ä½œ")
        sys.exit(0)
    except Exception as e:
        print(f"\nâŒ é”™è¯¯: {str(e)}")
        if args.verbose:
            import traceback
            traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()